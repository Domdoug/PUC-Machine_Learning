{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9O42xSiM150U"
   },
   "source": [
    "# Especialização em Ciência de Dados - PUC-Rio\n",
    "# Machine Learning\n",
    "## Exercício 4: Comparação de modelos de clusterização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I9DNy9kA2F7N"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "__Alunos:__ <br>\n",
    "        Cícero Felipe Ricci Bressane <br>\n",
    "        José Douglas Nascimento <br>\n",
    "        Ronaldo Costa <br>\n",
    "        Victor Mouffron Carvalho Machado\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uDTuZ9Y22NOO"
   },
   "source": [
    "# Comente, em português, cada uma das linhas (ou bloco de linhas) do exemplo abaixo de comparação de algoritmos de clustering. Em seguida, responda: O que você aprendeu neste exercício?#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZBSzVfx12k4H"
   },
   "source": [
    "__Nesse exercício foram criados diferentes datasets e aplicados diversos algoritmos de clusterização presentes na biblioteca sklearn. Cada linha da figura representa um dataset criado e cada coluna representa um algoritmo. Percebe-se que alguns algoritmos são superiores do que outros em determinados datasets. Apesar de percebermos algumas características desses algoritmos não podemos generalizar um desempenho superior para todos os datasets. Além disso, como indica a documentação do sklearn, apesar da intuição fornecida pela imagem, essa pode não prevalecer em dimensões maiores__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ACKJx8DIixI"
   },
   "source": [
    "# Comparing different clustering algorithms on toy datasets\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954
    },
    "colab_type": "code",
    "id": "YmDSxXxzIe6W",
    "outputId": "01939790-d50f-4503-ec08-0a5c10d3d721"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# importando as bibliotecas necessárias\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "np.random.seed(0) # fixando a semente para manter a reprodutibilidade do experimento\n",
    "\n",
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 1500 # número de amostras\n",
    "\n",
    "# criando datasets usando o sklearn para a clusterização \n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5, #criando círculos\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05) #criando \"luas\"\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)  #criando \"bolhas\"\n",
    "no_structure = np.random.rand(n_samples, 2), None  #criando dataset sem estrutura definida (aleatório)\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)\n",
    "\n",
    "# ============\n",
    "# Set up cluster parameters\n",
    "# ============\n",
    "plt.figure(figsize=(9 * 2 + 3, 12.5)) # definindo tamnho do gráfico\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {'quantile': .3,\n",
    "                'eps': .3,\n",
    "                'damping': .9,\n",
    "                'preference': -200,\n",
    "                'n_neighbors': 10,\n",
    "                'n_clusters': 3,\n",
    "                'min_samples': 20,\n",
    "                'xi': 0.05,\n",
    "                'min_cluster_size': 0.1}\n",
    "\n",
    "# criando lista com tuplas de datasets e parâmetros\n",
    "datasets = [\n",
    "    (noisy_circles, {'damping': .77, 'preference': -240,\n",
    "                     'quantile': .2, 'n_clusters': 2,\n",
    "                     'min_samples': 20, 'xi': 0.25}),\n",
    "    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),\n",
    "    (varied, {'eps': .18, 'n_neighbors': 2,\n",
    "              'min_samples': 5, 'xi': 0.035, 'min_cluster_size': .2}),\n",
    "    (aniso, {'eps': .15, 'n_neighbors': 2,\n",
    "             'min_samples': 20, 'xi': 0.1, 'min_cluster_size': .2}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets): # iterando no lista datasets\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset # desempacotando a tupla\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X) # normalizando o dataset\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    \n",
    "    # instanciando diferentes tipos de algoritmos de clusterização da biblioteca sklearn\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params['n_clusters'], linkage='ward',\n",
    "        connectivity=connectivity)\n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "        affinity=\"nearest_neighbors\")\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "    optics = cluster.OPTICS(min_samples=params['min_samples'],\n",
    "                            xi=params['xi'],\n",
    "                            min_cluster_size=params['min_cluster_size'])\n",
    "    affinity_propagation = cluster.AffinityPropagation(\n",
    "        damping=params['damping'], preference=params['preference'])\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\", affinity=\"cityblock\",\n",
    "        n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "    birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params['n_clusters'], covariance_type='full')\n",
    "    kmeans = cluster.KMeans(\n",
    "        n_clusters=params['n_clusters'], init='random')\n",
    "\n",
    "# criando tupla de tuplas com nome do algoritmo e instância criada\n",
    "    clustering_algorithms = (  \n",
    "        ('MiniBatchKMeans', two_means),\n",
    "        ('AffinityPropagation', affinity_propagation),\n",
    "        ('MeanShift', ms),\n",
    "        ('SpectralClustering', spectral),\n",
    "        ('Ward', ward),\n",
    "        ('AgglomerativeClustering', average_linkage),\n",
    "        ('DBSCAN', dbscan),\n",
    "        ('OPTICS', optics),\n",
    "        ('Birch', birch),\n",
    "        ('GaussianMixture', gmm),\n",
    "        ('K-Means', kmeans)\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \" +\n",
    "                \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\" +\n",
    "                \" may not work as expected.\",\n",
    "                category=UserWarning)\n",
    "            algorithm.fit(X) #iterando por cada algoritmo e treinando o cluster\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(np.int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num) # criando cada subplot\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        # definindo cores e outros elementos gráficos, como tamanho dos eixos\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        # add black color for outliers (if any)\n",
    "        colors = np.append(colors, [\"#000000\"])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show() #plota os gráficos criados"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ML - Exercício 4 - Comparação de modelos de clusterização.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
